% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{create_method}
\alias{create_method}
\title{Method creation for ensemble}
\usage{
create_method(
  method = c("mean", "ols", "ridge", "plasso", "forest_grf", "lasso", "knn",
    "forest_drf", "xgboost", "rlasso", "glm", "logit", "logit_nnet", "nb_gaussian",
    "nb_bernoulli", "xgboost_prop", "svm", "prob_forest", "ranger", "knn_prop"),
  x_select = NULL,
  arguments = list(),
  tuning = "no",
  multinomial = NULL,
  parallel = FALSE
)
}
\arguments{
\item{method}{Choose method from available options. See details for supported methods.}

\item{x_select}{Optional logical vector (length = number of covariates) indicating which
variables to use. For example, tree-based methods typically exclude the
interactions used by Lasso.}

\item{arguments}{Optional list of additional arguments passed to the underlying method.}

\item{tuning}{Hyperparameter tuning options for GRF's regression forest and XGBoost:
\describe{
  \item{\code{"full_sample"}}{Tune hyperparameters using full sample.}
  \item{\code{"fold"}}{Tuning is performed on the estimation part of the cross-fitting split.
                       Roughly \eqn{F} times more computationally intensive.}
  \item{\code{"no"}}{No tuning performed; uses default settings.}
}}

\item{multinomial}{Optional character specifying multiclass handling approach:
one of \code{c("one-vs-one", "one-vs-rest", "multiclass")}.}

\item{parallel}{Optional logical. If \code{TRUE}, enables parallelization for
the One-Vs-One (OvO) multiclass routine. Requires \code{foreach} and
\code{doParallel}.}
}
\value{
A list object that can be passed as input to \code{\link{ensemble}}.
}
\description{
Creates methods to be used in the subsequent \code{\link{ensemble}} model.
}
\details{
Supported methods include:

\itemize{
  \item \code{"mean"}: Mean difference.

  \item \code{"ols"}: Ordinary Least Squares.

  \item \code{"ridge"}: Ridge regression via \code{glmnet}. Uses 10-fold
  cross-validation (over 100 log-spaced values by default) to select the
  regularization strength \eqn{\lambda}. Users may supply a custom
  \eqn{\lambda} sequence, choose the CV loss, and set the number of folds.

  \item \code{"plasso"}: Post-Lasso estimator via \code{plasso} built on top of the 
  \code{glmnet} package, with tuning settings identical to Ridge.

  \item \code{"lasso"}: Standard Lasso via \code{glmnet}, with tuning settings
  identical to Ridge.

  \item \code{"rlasso"}: Lasso via \code{hdm} with a theory-driven,
  data-dependent penalty robust to heteroskedastic and non-Gaussian errors.
  By default, \code{rlasso()} includes sets the penalty with theoretical choices 
  (\eqn{c = 1.1}, \eqn{\gamma = 0.1 / \log(n)}).

  \item \code{"forest_grf"}: Regression forest via \code{grf}, with defaults
  \code{num.trees = 2000}, \code{min.node.size = 5},
  \code{sample.fraction = 0.5}, and \code{honesty = TRUE}.

  \itemize{
    \item If \code{tuning = "full_sample"}, tuning is performed on the
    full sample \eqn{(X,Y)} over \code{sample.fraction}, \code{mtry},
    \code{min.node.size}, \code{honesty.fraction}, \code{honesty.prune.leaves},
    \code{alpha}, and \code{imbalance.penalty}.

    \item If \code{tuning = "fold"}, tuning is performed on the
    estimation part of the cross-fitting split (\eqn{Fâ€“1} folds), which is roughly
    \eqn{F} times more computationally demanding.
  }

  \item \code{"xgboost"}: Gradient boosting via \code{xgboost}, using 100 
  boosting rounds by default. These hyperparameters are fixed to unsure smoother 
  extraction: \code{reg_alpha = 0}, \code{subsample = 1}, \code{max_delta_step = 0}, 
  \code{base_score = 0}. Supports the same tuning logic as regression
  forests. The Hyperband-like tuning routine and tunable hyperparameters are
  described in \code{?tune_xgb_hyperband}.

  \item \code{"knn"}: k-Nearest Neighbors via \code{FastKNN}, with
  \eqn{k = 10} neighbors by default.

  \item \code{"forest_drf"}: Distributional random forest via \code{drf}, with
  defaults \code{min.node.size = 15}, \code{num.trees = 2000},
  \code{splitting.rule = "FourierMMD"}.
  
  \item \code{"glm"}: Binary regression models using \code{glm}. Supports logistic
  (logit) and probit link functions. To use probit, specify 
  \code{family = binomial(link = "probit")}. Default is logit.

  \item \code{"logit"}: Logistic regression via \code{glmnet}. Uses
  \code{family = "binomial"} for binary outcomes and
  \code{family = "multinomial"} for multiclass outcomes.

  \item \code{"logit_nnet"}: Logistic regression via \code{nnet::multinom()},
  supports binary and multiclass outcomes.

  \item \code{"nb_gaussian"}: Gaussian Naive Bayes via
  \code{naivebayes::gaussian_naive_bayes()}.

  \item \code{"nb_bernoulli"}: Bernoulli Naive Bayes via
  \code{naivebayes::naive_bayes()} with Bernoulli likelihood.

  \item \code{"xgboost_prop"}: Gradient boosting via \code{xgboost} for
  classification or propensity score estimation. Uses
  \code{objective = "binary:logistic"} for binary outcomes and
  \code{objective = "multi:softprob"} for multiclass outcomes. Defaults to
  100 boosting rounds unless \code{nrounds} is specified.

  \item \code{"svm"}: Support Vector Machine via \code{e1071::svm()}.
  Defaults to \code{kernel = "radial"}, \code{type = "C-classification"},
  and enables probability estimates (\code{probability = TRUE}).
  
  \item \code{"prob_forest"}: Probability forest via
  \code{grf::probability_forest()}. Grows 2000 trees by default.

  \item \code{"ranger"}: Random forest classifier via \code{ranger}.
  Grows 500 trees by default.

  \item \code{"knn_prop"}: k-Nearest Neighbors classifier via
  \code{kknn::train.kknn()}.
}
}
\examples{
# Create list of methods for ensemble
methods <- list(
  "ols" = create_method("ols"),
  "knn" = create_method("knn", arguments = list("k" = 3)),
  "forest_grf" = create_method("forest_grf"),
  "logit_ovo" = create_method("logit", multinomial = "one-vs-one", parallel = TRUE),
  "prob_forest" = create_method("prob_forest", multinomial = "multiclass")
)
}
