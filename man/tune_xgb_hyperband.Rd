% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils.R
\name{tune_xgb_hyperband}
\alias{tune_xgb_hyperband}
\title{Hyperparameter Tuning for XGBoost via Hyperband}
\usage{
tune_xgb_hyperband(
  X,
  Y,
  max_rounds = 100,
  n_configs = 50,
  eta_downfactor = 3,
  nfold = 5,
  metrics = "rmse",
  seed = 123
)
}
\arguments{
\item{X}{Covariate matrix.}

\item{Y}{Numeric vector containing the outcome variable.}

\item{max_rounds}{Maximum number of boosting rounds for final evaluation. Default is 100.}

\item{n_configs}{Initial number of random hyperparameter configurations to sample. Default is 50.}

\item{eta_downfactor}{Downsampling factor between rungs (η in Hyperband terminology). Default is 3.}

\item{nfold}{Number of cross-validation folds. Default is 5.}

\item{metrics}{Evaluation metric for early stopping. Options include "rmse", "mae",
"logloss", "error", etc. Default is "rmse".}

\item{seed}{Random seed for reproducibility. Default is 123.}
}
\value{
A list with two components:
\itemize{
  \item{\code{params}}: Named list of optimal hyperparameter values
  \item{\code{best_score}}: The best cross-validation score achieved (minimum test error)
}
}
\description{
Implements the Hyperband multi-armed bandit for hyperparameter optimization.
Progressively allocates resources (boosting rounds)
to the most promising configurations across successive rungs of evaluation.
}
\details{
The function implements the Hyperband algorithm with three resource levels (rungs):
\enumerate{
  \item \strong{First rung:} 10 boosting rounds - evaluate all `n_configs` configurations
  \item \strong{Second rung:} 30 boosting rounds - keep top 1/η configurations
  \item \strong{Final rung:} `max_rounds` boosting rounds - keep top configuration
}

Tunable hyperparameters include learning rate (eta), tree depth, regularization terms,
and subsampling parameters. The search space is designed for general-purpose optimization
with log-uniform sampling for continuous parameters.
}
\keyword{internal}
