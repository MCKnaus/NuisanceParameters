% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_learners.r
\name{xgboost_tune}
\alias{xgboost_tune}
\title{Hyperparameter Tuning for XGBoost via Hyperband}
\usage{
xgboost_tune(
  X,
  Y,
  max_rounds = 100,
  n_configs = 50,
  eta_downfactor = 3,
  nfold = 5,
  metrics = "rmse",
  seed = 123
)
}
\arguments{
\item{X}{Covariate matrix.}

\item{Y}{Numeric vector containing the outcome variable.}

\item{max_rounds}{Maximum number of boosting rounds for final evaluation. Default is 100.}

\item{n_configs}{Initial number of random hyperparameter configurations to sample. Default is 50.}

\item{eta_downfactor}{Downsampling factor between rungs (η in Hyperband terminology). Default is 3.}

\item{nfold}{Number of cross-validation folds. Default is 5.}

\item{metrics}{Evaluation metric for early stopping. Options include "rmse", "mae",
"logloss", "error", etc. Default is "rmse".}

\item{seed}{Random seed. Default is 123.}
}
\value{
A list with two components:
\itemize{
  \item{\code{params}}: Named list of optimal hyperparameter values
  \item{\code{best_score}}: The best cross-validation score achieved (minimum test error)
}
}
\description{
Implements the Hyperband multi-armed bandit for hyperparameter optimization.
Progressively allocates resources (boosting rounds)
to the most promising configurations across successive rungs of evaluation.
}
\details{
The function implements the Hyperband algorithm with three resource levels (rungs):
\enumerate{
  \item \strong{First rung:} 10 boosting rounds - evaluate all `n_configs` configurations
  \item \strong{Second rung:} 30 boosting rounds - keep top 1/η configurations
  \item \strong{Final rung:} `max_rounds` boosting rounds - keep top configuration
}

Tunable hyperparameters include the learning rate (\code{eta}), tree depth (\code{max_depth}),
minimum child weight (\code{min_child_weight}), \code{gamma}, column subsampling fractions
(\code{colsample_bytree}, \code{colsample_bylevel}, \code{colsample_bynode}),
L2 regularization (\code{lambda}), \code{grow_policy}, \code{max_leaves}, and \code{max_bin}.
The search space uses log-uniform sampling for \code{eta} and \code{lambda},
and uniform or discrete sampling for the others. Parameters kept fixed are:
\code{subsample = 1}, \code{alpha = 0}, \code{max_delta_step = 0},
\code{tree_method = "hist"}, \code{objective = "reg:squarederror"},
and \code{base_score = mean(Y)}.
}
\keyword{internal}
