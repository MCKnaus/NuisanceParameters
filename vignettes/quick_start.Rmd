---
title: "NuisanceParameters: Quick Start"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "10/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{NuisanceParameters: Quick Start}
  %\VignetteEngine{knitr::rmarkdown}
---

## Introduction

This notebook offers a general introduction to `NuisanceParameters`.

It covers:

1.  Data preprocessing and base learner setup
2.  Estimation with multiple machine learners using short-stacking
3.  Adding or replacing nuisance parameters ex-post
4.  Target parameter estimation and smoother matrix extraction (will be delegated to other packages in the trilogy)

Find more advanced topics in the *Stacking*, *Hyperparameter Tuning*, and *Multivalued Treatment* vignettes.

## **Load packages and data**

Load the relevant packages

```{r}
library(hdm)
library(OutcomeWeights)
library(NuisanceParameters)
```

The notebook builds on the dataset provided in the `hdm` package. The data was used in Chernozhukov and Hansen (2004) and Belloni et al. (2014) where further details can be found. They investigate the effect of participation in the employer-sponsored 401(k) retirement savings plan on net assets (`net_tfa`), using 401(k) eligibility (`e401`) as an instrument for 401(k) participation (`p401`).

The sample contains 9,915 observations. We use a 25% subset to speed up computation:

```{r}
# Find dataset description if you type ?pension in console
data("pension")

set.seed(123)
idx <- sample(nrow(pension), size = round(0.25 * nrow(pension)))
sub <- pension[idx, ]

D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X_raw <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc
                      + male + marr + pira + twoearn, data = sub)
```

The covariate matrix can be preprocessed using the `design_matrix` function, which expands it with interactions, polynomials, and logs:

```{r}
# see ?design_matrix
X_big <- design_matrix(data = X_raw, int = "all", int_d = 2, poly = "all", poly_d = 2)
dim(X_big)
```

Since this expansion can create highly correlated covariates, `data_screen` helps clean the data and improve performance in prediction algorithms.

```{r}
# see ?data_screen
X <- data_screen(X_big, treat = D, bin_cut = 0.02, corr_cut = 0.9, quiet = FALSE)
```

Next, we specify a list of base learners for the ensemble. The package offers a wide variety of choices—you can find the full list of available learners and their default settings by typing `?create_method` in the console. For more details on the different types of stacking and hyperparameter tuning, see the *Stacking* and *Hyperparameter Tuning* notebooks.

The logic is simple: choose a base learner from the available methods, supply any optional method-specific arguments (for example, `num.trees` for grf’s forest), and, if desired, specify a subset of covariates to be used in estimation.

```{r}
# For tree methods, restrict to original features
cols <- colnames(X) %in% colnames(X_raw)

methods = list(
 "ols" = create_method("ols"),
 "plasso" = create_method("plasso"),
 "forest_grf" = create_method("forest_grf", x_select = cols, 
                              arguments = list("num.trees" = 1000, "min.node.size" = 5)),
 "xgboost" = create_method("xgboost", x_select = cols,
                           arguments = list("nrounds" = 150)),
 "xgboost_tuned" = create_method("xgboost", x_select = cols, tuning = "full_sample")
 )
```

## Estimate nuisance parameters

We are now all set to run the main function.

`nuisance_parameters` carries out cross-fitted estimation of nuisance parameters. If more than one method is provided, it will stack them together, creating an ensemble learner.

In this example, we use 5-fold cross-fitting (4/5 of the observations are used to train the machine learner in each cross-fitting split) and keep treatment ratios balanced within the folds (`stratify = TRUE`).

Short stacking is applied:

```{r}
np <- nuisance_parameters(NuPa = c("Y.hat","Y.hat.d","D.hat","Z.hat"),
                          X = X, Y = Y, D = D, Z = Z,
                          methods = methods, cf = 5, stacking = "short",
                          cluster = NULL, stratify = TRUE, store_models = "memory")
```

Estimated nuisance parameters can be immediately inspected:

```{r}
lapply(np$nuisance_parameters, head)
```

Along with the ensemble weights in the short-stacking procedure. By default, these are obtained by regressing the outcome (and similarly the treatment and instrument) on cross-fitted base learner predictions using non-negative least squares. The weights are normalized to sum to one:

```{r}
plot(np$numbers$ens_weights)
```

Now, let's estimate some target parameters using the early `MLeffect` function (to be outsourced).

```{r}
TargetParam <- MLeffect(Y, D, X, Z, 
                        NuPa.hat = np$nuisance_parameters,
                        estimators = c("PLR","PLR_IV","AIPW_ATE"))

summary(TargetParam)
plot(TargetParam)
```

## Adding nuisance parameters ex-post

Nuisance parameters (and the respective trained models) can be easily replaced or added to a `NuisanceParameters` object.

For example, you might want to adjust your estimation strategy by using a different set of base learners:

```{r}
new_methods = list(
 "knn" = create_method("knn"),
 "ridge" = create_method("ridge"),
 "xgboost" = create_method("xgboost", x_select = cols)
 )
```

The only important detail is to keep using the same cross-fit indicator matrix from the original call. Notice the additional `cf_mat` argument:

```{r}
np_2 <- nuisance_parameters(NuPa = c("Y.hat","Z.hat"),
                            X = X, Y = Y, D = D, Z = Z, cf_mat = np$numbers$cf_mat,
                            methods = new_methods, cf = 5, stacking = "short",
                            cluster = NULL, stratify = TRUE, store_models = "memory")

plot(np_2$numbers$ens_weights)
```

Use `add_nupa` to combine or replace different `NuisanceParameters` objects:

```{r}
np_new <- add_nupa(np = np, np_new = np_2, NuPa = c("Y.hat","Z.hat"), replace = TRUE)
plot(np_new$numbers$ens_weights)
```

## Extracting smoother matrices

A key feature of this package pipeline (`NuisanceParameters → MLeffects → OutcomeWeights`) is the ability to extract smoothers from the underlying trained models.

Smoother matrices are $N \times N$ objects that generate outcome predictions by weighting observed outcomes: $\widehat{Y}_i = \sum^N_{j=1} s_{i \leftarrow j} Y_j$. Each entry $s_{i \leftarrow j}$ shows how much unit $j$’s outcome contributes to predicting unit $i$. A familiar example of a smoother is the OLS projection matrix.

Smoothers let us express predictions as linear transformations of outcomes: this makes it possible to define outcome weights $\omega$ for complex machine learning estimators (like causal forests) and connect them to familiar diagnostic tools such as covariate balance checks. For details, see [Knaus (2024)](https://arxiv.org/abs/2411.11559).

Use `get_smoother_weights` from the `OutcomeWeights` package to extract outcome smoother matrices using the trained models saved in the previous step:

```{r}
# Delivers smoother matrices for each outcome nuisance parameter
S <- get_smoother_weights(object = np, NuPa = "Y.hat")

# Check if S*Y = Y_hat
isTRUE(all.equal(as.numeric(S$Y.hat %*% Y), 
                 np$nuisance_parameters$Y.hat, 
                 tolerance = 1e-7))
```
