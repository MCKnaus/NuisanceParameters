---
title: "Multivalued Treatments"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "09/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Estimation}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r}
library(conflicted)
invisible(lapply(c("create_method", "design_matrix", "data_screen"), 
       function(x) conflict_prefer(x, "NuisanceParameters", quiet = TRUE)))

library(causalDML)
library(NuisanceParameters)
```

## Introduction

This notebook introduces methods for estimating nuisance parameters with multivalued treatments, focusing on:

-   estimating generalized propensity scores: multiclass, One-vs-Rest (OvR), and One-vs-One (OvO)\
-   ensembling techniques: stacked non-negative least squares and the BFGS algorithm\
-   an application to the Pennsylvania re-employment bonus experiment

## Estimating propensity scores for multivalued treatments

When propensity scores need to be estimated for treatments with more than two values, algorithms originally designed for binary tasks can still be applied. Two common approaches are One-vs-Rest (OvR) and One-vs-One (OvO).

**One-vs-Rest (OvR)** reduces a multi-class problem to a series of binary tasks. For example, with categories A, B, and C, one model distinguishes A from (B and C), another B from (A and C), and a third C from (A and B). Each model is trained on the full dataset, with outcomes recoded as “this class” versus “the rest.” Predictions from all models are then normalized to yield class probabilities.

**One-vs-One (OvO)** instead builds a separate classifier for every pair of classes. With A, B, and C, one model distinguishes A from B, another A from C, and a third B from C. In general, $K(K-1)/2$ models are trained, each using only the relevant subset of data. Final predictions are obtained by combining the pairwise probabilities, often through Kullback–Leibler (KL) distance minimization. Note that the number of models grows quickly as $K$ increases—for instance, 13 classes require 78 classifiers.

Another, and often preferred, approach is to use algorithms that natively support multivalued treatments. This can be done by setting `multinomial = "multiclass"`.

## Obtaining ensemble weights

With multivalued treatments, each learner produces an array of size $N \times K \times M$, where $K$ is the number of treatments and $M$ the number of models. Each row sums to one across treatment states.

This setup rules out standard ensembling methods like OLS, since they would violate the probability constraint. The package provides two alternatives.

The first is stacked NNLS: all treatment-specific estimates are stacked across $K$, a single outcome vector is formed, and $X$ is flattened into a $(KN \times M)$ matrix, on which `nnls` is applied.

The second, enabled by setting `do_bfgs = TRUE`, uses a quasi-Newton optimization (BFGS) to approximate the Hessian and find local minima of smooth, unconstrained nonlinear functions. For multinomial outcomes, BFGS estimates ensemble weights under a softmax constraint, using mean squared error (the Brier score) as the loss function.

## Pennsylvania re-employment bonus experiment

We revisit the Pennsylvania re-employment bonus experiment ([Bilias, 2000](https://onlinelibrary.wiley.com/doi/10.1002/jae.579)).

These experiments were conducted in the 1980s by the U.S. Department of Labor to test the incentive effects of alternative compensation schemes for unemployment insurance (UI): UI claimants were randomly assigned either to a control group or one of six treatments.

All treatments offered a cash bonus for finding a job quickly, but varied three factors: the bonus size (low = 3x or high = 6x the weekly benefit amount), the time limit to qualify (short = 6 weeks or long = 12 weeks), and whether a job search workshop was offered. The control group received the standard unemployment benefits with no bonus or workshop.

$X$ includes demographics, enrollment details, and occupation; $Y$ is the first unemployment spell in weeks. You can learn more about the data by typing `?pa_reemployment` in the console.

```{r}
data(pa_reemployment)
Penn <- pa_reemployment

Y <- log(Penn$inuidur1)
D <- Penn$tg
X_raw <- model.matrix(~0 + female + black + hispanic + othrace + dep + q2 
                  + q3 + q4 + q5 + q6 + recall + agelt35 + agegt54 
                  + durable + nondurable + lusd + husd + muld, data = Penn)

X <- data_screen(data = X_raw, treat = D, bin_cut = 0.02, corr_cut = 0.9, quiet = FALSE)
```

We define a flexible set of base learners. For propensity score estimation, we explore all available multiclass strategies, and the One-vs-One specification is parallelized. See `?create_method` for details on parallelization.

We also set `stratify = TRUE` so that cross-fitting folds preserve the treatment ratios from the full sample.

```{r}
methods = list(
 "ols" = create_method("ols"),
 "plasso" = create_method("plasso"),
 "forest_grf" = create_method("forest_grf"),
 "logit" = create_method("logit", multinomial = "multiclass"),
 "logit_nnet" = create_method("logit_nnet", multinomial = "one-vs-rest"),
 "prob_forest" = create_method("prob_forest", multinomial = "multiclass"),
 "ranger" = create_method("ranger", multinomial = "one-vs-one", parallel = TRUE)
 )

np <- nuisance_parameters(NuPa = c("Y.hat.d", "D.hat"), X = X, Y = Y, D = D,
                          methods = methods, cf = 3, stacking = "short", 
                          stratify = TRUE, storeModels = "No")

plot(np$numbers$ens_weights)
```

By default, ensemble weights for multivalued treatments are estimated via non-negative least squares (`nnls`) on stacked predictions. Alternatively, BFGS optimization can be used.

```{r}
np <- nuisance_parameters(NuPa = c("Y.hat.d", "D.hat"), X = X, Y = Y, D = D,
                          methods = methods, cf = 3, stacking = "short",
                          stratify = TRUE, storeModels = "No", do_bfgs = TRUE)

plot(np$numbers$ens_weights)
```

We plug the estimated nuisance parameters in the doubly-robust AIPW score to estimate the target parameter: the average treatment effect of receiving any of the six effective treatments.

[causalDML](https://github.com/MCKnaus/causalDML) package conveniently provides all the necessary infrastructure to achieve this: `DML_aipw` allows us to pass already estimated outcome and treatment nuisance parameters.

```{r}
# see ?causalDML::DML_aipw
DML = DML_aipw(y = Y, w = D, x = X, 
                e_mat = np$nuisance_parameters$D.hat, 
                m_mat = np$nuisance_parameters$Y.hat.d
                )

d_label <- c("Control",
             "Low_Short_WS",
             "Low_Long_WS",
             "High_Short_WS",
             "High_Long_WS",
             "InitHigh_Long_WS",
             "High_Long_NoWS")

plot(DML$APO, label = d_label)
```

The ATEs represent the percentage decrease in unemployment spells resulting from program participation:

```{r}
DML$ATE$results[1:6,]
```

We find results consistent with the study’s conclusion that reemployment bonuses had limited overall effects on accelerating job finding or reducing UI duration.
