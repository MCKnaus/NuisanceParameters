---
title: "NuisanceParameters - progress as of 26.06"
subtitle: ""
author: "Roman Rakov"
date: "06/25"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

This notebook shows some functionality of the `NuisanceParameters` package.

## Getting started

First, load packages and set the seed:

```{r}
if (!require("OutcomeWeights")) install.packages("OutcomeWeights", dependencies = TRUE); library(OutcomeWeights)
if (!require("hdm")) install.packages("hdm", dependencies = TRUE); library(hdm)
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE); library(dplyr)
if (!require("purrr")) install.packages("purrr", dependencies = TRUE); library(purrr)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE); library(ggplot2)

#devtools::document()
library(NuisanceParameters)

set.seed(1234)
```

Define some useful functions:

```{r}
ens_weights <- function(models_data, learner_names) {
  v1 <- compact(map_depth(models_data, 1, "nnls_w", .ragged = TRUE))
  v2 <- compact(map_depth(models_data, 2, "nnls_w", .ragged = TRUE))
  v2$Y.hat_ml <- v1$Y.hat_ml
  
  v2 %>% imap(~ if(is.list(.x) && !is.null(names(.x[[1]]))) {
    setNames(.x, paste0(.y, "[[", seq_along(.x), "]]"))
    } else setNames(list(.x), .y)) %>% flatten() %>% map(~ setNames(.x, learner_names))
}
```

Next, load the data. Here we use a sample from the 401(k) data of the `hdm` package.

```{r}
data(pension)
idx <- sample(nrow(pension), size = round(0.1 * nrow(pension)))
sub <- pension[idx, ]

# subset
W <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn,
                  data = sub)
```

Now we structure the inputs.

Starting with the instrument and treatment indicator matrices:

```{r}
w_mat = prep_w_mat(W)
z_mat = prep_w_mat(Z)

head(w_mat)
head(z_mat)
```

We prepare these because `w_mat` can be passed to `prep_cf_mat`, which is used in both nuisance parameter estimation and smoother matrix recovery—two separate steps—so it needs to be saved outside `nuisance_parameters`.

That said, it could also be done internally: you just pass `W`, `Z`, and `cf`, and `nuisance_parameters` handles the rest, returning `cf_mat`.

Here, we do it manually:

```{r}
# Cross-fitting fold indicators
cf_mat = prep_cf_mat(n=length(Y), cf = 5, w_mat = w_mat, cl = NULL)
head(cf_mat)
```

**Reminder**: `cf_mat` creates a matrix of binary cross-fitting fold indicators (n x cross-folds). It can account for clusters within the data (specify `cl`) and preserve the treatment ratios from full sample (specify `w_mat`). If both are provided, `w_mat` is ignored due to computational constraints and feasibility issues.

Along with the estimated models, `cf_mat` will later be passed to `get_outcome_weights`.

Lastly, we specify the methods used in the ensemble with `create_method`:

```{r}
ml = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf")
)
```

## Run `nuisance_parameters`

Now, we are ready to run the main function:

```{r}
# Short stacking 
cv = 1 

# nuisance parameter estimation
np_short <- nuisance_parameters(NuPa = c("Y.hat","Yw.hat","Yz.hat","W.hat", "Wz.hat", "Z.hat"),
                          ml = ml, x = X, y = Y, w = W, z = Z,
                          cf_mat = cf_mat, w_mat = w_mat, z_mat = z_mat, 
                          cv = cv, learner = c("t"), 
                          storeModels = "Disk", path = "/Users/romanrakov/Desktop/save", quiet = TRUE)
```

To make sure the result makes sense, let's manually estimate some target parameters and compare the result to `dml_with_smoother` of `OutcomeWeights`:

```{r}
ehat = mean(W)

mxhat = np_short$nuisance_parameters$Y.hat
exhat = np_short$nuisance_parameters$W.hat
hhat  = np_short$nuisance_parameters$Z.hat

mwhat0 = np_short$nuisance_parameters$Yw.hat[,1]
mwhat1 = np_short$nuisance_parameters$Yw.hat[,2]

mzhat0 = np_short$nuisance_parameters$Yz.hat[,1]
mzhat1 = np_short$nuisance_parameters$Yz.hat[,2]

ezhat0 = np_short$nuisance_parameters$Wz.hat[,1]
ezhat1 = np_short$nuisance_parameters$Wz.hat[,2]

pa_pl = -(W - exhat)^2
pb_pl = (Y - mxhat) * (W - exhat)

pa_ate = rep(-1,length(Y))
pb_ate = mwhat1 - mwhat0 + W * (Y - mwhat1) / ehat - (1 - W) * (Y - mwhat0) / (1-ehat)

pa_iv = -(W - exhat) * (Z - hhat)
pb_iv = (Y - mxhat) * (Z - hhat) 

pa_late = -( ezhat1 - ezhat0 + Z * (W - ezhat1) / hhat - (1 - Z) * (W - ezhat0) / (1-hhat) )
pb_late = mzhat1 - mzhat0 + Z * (Y - mzhat1) / hhat - (1 - Z) * (Y - mzhat0) / (1-hhat)

DML_inference = function(psi_a,psi_b) {
  N = length(psi_a)
  theta = -sum(psi_b) / sum(psi_a)
  psi = theta * psi_a + psi_b
  Psi = - psi / mean(psi_a)
  sigma2 = var(Psi)
  se = sqrt(sigma2 / N)
  t = theta / se
  p = 2 * pt(abs(t),N,lower = FALSE)
  result = c(theta,se,t,p)
  return(result)
}
```

Short stacking 2-learner estimation:

```{r}
results = matrix(NA,4,4)
rownames(results) = c("PLR","PLR-IV","AIPW-ATE","Wald-AIPW")
colnames(results) = c("Effect","S.E.","t","p")
results[1,] = DML_inference(pa_pl,pb_pl)
results[2,] = DML_inference(pa_iv,pb_iv)
results[3,] = DML_inference(pa_ate,pb_ate)
results[4,] = DML_inference(pa_late,pb_late)

printCoefmat(results,has.Pvalue = TRUE)

data.frame(
  thetas = results[, 1],ses = results[, 2],Estimator = rownames(results),
  cil = results[, 1] - 1.96 * results[, 2],ciu = results[, 1] + 1.96 * results[, 2]) %>% 
  mutate(Estimator = factor(Estimator, levels = rownames(results))) %>% 
  ggplot(aes(x = Estimator, y = thetas, ymin = cil, ymax = ciu)) + 
  geom_point(size = 2.5) + geom_errorbar(width = 0.15) + geom_hline(yintercept = 0)
```

And compare it to:

```{r}
dml_5f = dml_with_smoother(Y,W,X,Z, n_cf_folds = 5)
results_dml_5f = summary(dml_5f)
plot(dml_5f)
```

We can also study the ensemble weights:

```{r}
ens_weights(np_short$models, names(ml))
```

## Extract smoother matrices S

Here comes the second function, which will later move to the `OutcomeWeights` package:

```{r}
S <- NuisanceParameters::get_outcome_weights(np_object = np_short, 
                         ml = ml, 
                         x = X, y = Y, w_mat=w_mat, z_mat=z_mat,
                         cv = cv, cf_mat = cf_mat,
                         NuPa = c("Y.hat","Yw.hat","Yz.hat","W.hat", "Wz.hat", "Z.hat"),
                         quiet=TRUE)

# Check if SY = Y_hat
all.equal(all.equal(as.numeric(S$Y.hat_ml%*%Y), np_short$nuisance_parameters$Y.hat), 
          all.equal(as.numeric(S$Yw.hat_ml[[1]]%*%Y), np_short$nuisance_parameters$Yw.hat[,1]),
          all.equal(as.numeric(S$Yw.hat_ml[[2]]%*%Y), np_short$nuisance_parameters$Yw.hat[,2]),
          all.equal(as.numeric(S$Yz.hat_ml[[1]]%*%Y), np_short$nuisance_parameters$Yz.hat[,1]),
          all.equal(as.numeric(S$Yz.hat_ml[[2]]%*%Y), np_short$nuisance_parameters$Yz.hat[,2]))
```

At this point, this function is a bit of a misnomer, providing smoother matrices instead of outcome weights. In the future, it will also take in the target parameters estimated in MLeffects or somewhere else - and provide a vector of outcome weights.

Stripping the weight functionality (which was creating smoother matrices) away from the nuisance_parameters routine made the code much less elegant.

To double check: do we expect smoother extraction within nuisance_parameters and within get_outcome_weights?

Now, let's repeat for standard stacking and print the progress.

The progress bar looks amazing in the concole, but something breaks when knitting a notebook. I will replace it with something better looking, because I don't seem to find a way to get rid of these extra "G3" symbols.

I like it because it provides more information on what exactly is being done: adapt from work done in each cross-validation fold (`cv=1,2..`) `cv = fs` run all methods on the full sample (fs) and and `cv = op` is out of sample prediction (also on full sample?)

```{r}
# Standard stacking 
cv = 2

# nuisance parameter estimation
np_stand <- nuisance_parameters(NuPa = c("Y.hat","Yw.hat","Yz.hat","W.hat", "Wz.hat", "Z.hat"),
                          ml = ml, x = X, y = Y, w = W, z = Z,
                          cf_mat = cf_mat, w_mat = w_mat, z_mat = z_mat, 
                          cv = cv, learner = c("t"), 
                          storeModels = "Memory", path = NULL, quiet = FALSE)
```

And extract the smoother matrices for every outcome model:

```{r}
S2 <- NuisanceParameters::get_outcome_weights(np_object = np_stand,
                         ml = ml, 
                         x = X, y = Y, w_mat=w_mat, z_mat=z_mat,
                         cv = cv, cf_mat = cf_mat,
                         NuPa = c("Y.hat","Yw.hat","Yz.hat","W.hat", "Wz.hat", "Z.hat"),
                         quiet=TRUE)

# Check if they are all correct
all.equal(all.equal(as.numeric(S2$Y.hat_ml%*%Y), np_stand$nuisance_parameters$Y.hat), 
        all.equal(as.numeric(S2$Yw.hat_ml[[1]]%*%Y), np_stand$nuisance_parameters$Yw.hat[,1]), 
        all.equal(as.numeric(S2$Yw.hat_ml[[2]]%*%Y), np_stand$nuisance_parameters$Yw.hat[,2]), 
        all.equal(as.numeric(S2$Yz.hat_ml[[1]]%*%Y), np_stand$nuisance_parameters$Yz.hat[,1]), 
        all.equal(as.numeric(S2$Yz.hat_ml[[2]]%*%Y), np_stand$nuisance_parameters$Yz.hat[,2]))
```
