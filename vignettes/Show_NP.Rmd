---
title: "NuisanceParameters - progress as of 26.06"
subtitle: ""
author: "Roman Rakov"
date: "06/25"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

This notebook shows some functionality of the `NuisanceParameters` package.

## Getting started

First, load packages and set the seed:

```{r}
if (!require("OutcomeWeights")) install.packages("OutcomeWeights", dependencies = TRUE); library(OutcomeWeights)
if (!require("hdm")) install.packages("hdm", dependencies = TRUE); library(hdm)
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE); library(dplyr)
if (!require("purrr")) install.packages("purrr", dependencies = TRUE); library(purrr)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE); library(ggplot2)

#devtools::document()

set.seed(1234)
```

Next, load the data. Here we use a sample from the 401(k) data of the `hdm` package.

```{r}
data(pension)
idx <- sample(nrow(pension), size = round(0.1 * nrow(pension)))
sub <- pension[idx, ]

# subset
D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn,
                  data = sub)
```

We just pass `W`, `Z`, and `cf`, and `nuisance_parameters` handles the rest, returning `cf_mat`.

**Reminder**: `cf_mat` creates a matrix of binary cross-fitting fold indicators (n x cross-folds). It can account for clusters within the data (specify `cl`) and preserve the treatment ratios from full sample (specify `w_mat`). If both are provided, `w_mat` is ignored due to computational constraints and feasibility issues.

Along with the estimated models, `cf_mat` will later be passed to `get_outcome_weights`.

Lastly, we specify the methods used in the ensemble with `create_method`:

```{r}
ml = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf"),
 # "lasso" = create_method("lasso"),
 # "ridge" = create_method("ridge"),
 "knn" = create_method("knn", arguments = list("k" = 3))
)
```

## Run `nuisance_parameters`

Now, we are ready to run the main function:

```{r}
# Short stacking 
time_short_notune <- system.time({
np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z,
                                ml = ml, cf = 5, stacking = "short",
                                cluster = NULL, stratify = FALSE, learner = c("t"), 
                                storeModels = "Memory", path = NULL, quiet = TRUE)})
```

To be sure the result makes sense, let's estimate some target parameters using early `MLeffect` function and compare them to `dml_with_smoother` of `OutcomeWeights`:

```{r}
TargetParam <- MLeffect(Y, D, X, Z, 
                        NuPa.hat = np_short$nuisance_parameters,
                        estimators = c("PLR","PLR_IV","AIPW_ATE","Wald_AIPW"))

summary(TargetParam)
plot(TargetParam)
```

We work with only a fraction of `401k`, so the estimates are noisy.

Observe that the results are very close to those from the 5-fold `dml_with_smoother` routine:

```{r}
dml_5f = dml_with_smoother(Y,D,X,Z, n_cf_folds = 5)
results_dml_5f = summary(dml_5f)
plot(dml_5f)
```

We can also check the ensemble weights for short stacking:

```{r}
plot(np_short$numbers$ens_weights)
```

## Extract smoother matrices S

Here comes the second function, which will later move to the `OutcomeWeights` package:

```{r}
S <- NuisanceParameters::get_outcome_weights(
  np_object = np_short, 
  NuPa = c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"))

# Check if SY = Y_hat
checks <- c(
  all.equal(as.numeric(S$Y.hat_ml %*% Y), np_short$nuisance_parameters$Y.hat),
  all.equal(as.numeric(S$Y.hat.d_ml[[1]] %*% Y), np_short$nuisance_parameters$Y.hat.d[, 1]),
  all.equal(as.numeric(S$Y.hat.d_ml[[2]] %*% Y), np_short$nuisance_parameters$Y.hat.d[, 2]),
  all.equal(as.numeric(S$Y.hat.z_ml[[1]] %*% Y), np_short$nuisance_parameters$Y.hat.z[, 1]),
  all.equal(as.numeric(S$Y.hat.z_ml[[2]] %*% Y), np_short$nuisance_parameters$Y.hat.z[, 2]))

all(sapply(checks, isTRUE))
```

At this point, this function is a bit of a misnomer: it gives smoother matrices instead of outcome weights. In the future, it will also take in the target parameters estimated in `MLeffects` or elsewhere—and provide a vector of outcome weights.

Stripping the "weight" functionality (which was creating smoother matrices) away from the `nuisance_parameters` routine made the code much less elegant.

### Standard stacking

Now, let's repeat for standard stacking and print the progress. Here, we first save it to disk and then load it back in to check that everything works:

```{r}
# Standard stacking 
time_stand_notune <- system.time({
np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat.z","D.hat","Z.hat"),
                                X, Y, D, Z,
                                ml = ml, cf = 5, stacking = 2,
                                cluster = NULL, stratify = FALSE, learner = c("t"), 
                                storeModels = "Disk", quiet = FALSE, 
                                path = "/Users/romanrakov/Desktop")})
```

The progress bar looks amazing in the console, but something breaks when knitting a notebook.

It is useful for giving more information on what exactly is being done:

-   `cv = 1, 2, ...`: Each candidate learner *j* is trained on the data excluding the current cross-validation hold-out fold, i.e., $T_k \setminus T_{k,v}$ in the notation of Ahrens et al. (2024)

-   `cv = .`: After cross-validating the individual learners within each cross-fitting fold, we re-fit our learners on the full training portion of the cross-fitting fold (full sample, fs)

-   `cv = .`: We use the learners fitted in `cv = .` and make predictions on the held-out part of the cross-fitting fold using ensemble weights obtained from `cv = 1, 2, ...`.

```{r}
S2 <- NuisanceParameters::get_outcome_weights(
  np_object = "/Users/romanrakov/Desktop/nuisance_models.rds", 
  NuPa = c("Y.hat","Y.hat.d","Y.hat.z"))

# Check if SY = Y_hat
checks <- c(
  all.equal(as.numeric(S2$Y.hat_ml %*% Y), np_stand$nuisance_parameters$Y.hat),
  all.equal(as.numeric(S2$Y.hat.d_ml[[1]] %*% Y), np_stand$nuisance_parameters$Y.hat.d[, 1]),
  all.equal(as.numeric(S2$Y.hat.d_ml[[2]] %*% Y), np_stand$nuisance_parameters$Y.hat.d[, 2]),
  all.equal(as.numeric(S2$Y.hat.z_ml[[1]] %*% Y), np_stand$nuisance_parameters$Y.hat.z[, 1]),
  all.equal(as.numeric(S2$Y.hat.z_ml[[2]] %*% Y), np_stand$nuisance_parameters$Y.hat.z[, 2]))

all(sapply(checks, isTRUE))
```

We can also display the weights for standard stacking. The format is slightly different: for each nuisance parameter, weights are computed for every cross-fitting fold:

```{r}
plot(np_stand$numbers$ens_weights)
```

## Hyperparameter tuning

As of now, only `grf`'s regression forest can be tuned.

It can be done on full sample by specifying `arguments = list("tune_full")`..

```{r}
ml = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf", arguments = list("tune_full"))
)

time_short_tune_full <- system.time({
np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z, ml = ml, cf = 5, stacking = "short",
                                learner = c("t"), storeModels = "Memory")})

time_stand_tune_full <- system.time({
np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z, ml = ml, cf = 5, stacking = 2,
                                learner = c("t"), storeModels = "Memory")})
```

Or on the fold by specifying `arguments = list("tune_fold")`..

```{r}
ml = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf", arguments = list("tune_fold"))
)

# Measure execution time
time_short_tune_fold <- system.time({
np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z, ml = ml, cf = 5, stacking = "short",
                                learner = c("t"), storeModels = "No")})

time_stand_tune_fold <- system.time({
np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z, ml = ml, cf = 5, stacking = 2,
                                learner = c("t"), storeModels = "No")})
```

The warnings appear because there is very little variation in $D$ when $Z = 0$.

And print the training time:

```{r}
cat("
Short stacking not tuned:    ", time_short_notune[3], "s
Standard stacking not tuned: ", time_stand_notune[3], "s
\nShort stacking full-tuned:   ", time_short_tune_full[3], "s  
Standard stacking full-tuned:", time_stand_tune_full[3], "s
\nShort stacking fold-tuned:   ", time_short_tune_fold[3], "s  
Standard stacking fold-tuned:", time_stand_tune_fold[3], "s"
    )
```

This is it. This notebook will become a vignette when it’s ready.
