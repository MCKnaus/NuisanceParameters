---
title: "NuisanceParameters"
subtitle: "Version: July 9th 2025"
author: "Roman Rakov"
date: "07/25"
output: 
  html_notebook:
    toc: true
    toc_float: true
    code_folding: show
---

*The notebook is under development.*

This notebook shows some functionality of the `NuisanceParameters` package.

## Getting started

First, load packages and set the seed:

```{r}
if (!require("OutcomeWeights")) install.packages("OutcomeWeights", dependencies = TRUE); library(OutcomeWeights)
if (!require("hdm")) install.packages("hdm", dependencies = TRUE); library(hdm)
if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE); library(dplyr)
if (!require("purrr")) install.packages("purrr", dependencies = TRUE); library(purrr)
if (!require("ggplot2")) install.packages("ggplot2", dependencies = TRUE); library(ggplot2)

# devtools::document()

set.seed(123)
```

Next, load the data. Here we use a sample from the 401(k) data of the `hdm` package.

```{r}
data(pension)
idx <- sample(nrow(pension), size = round(0.15 * nrow(pension)))
sub <- pension[idx, ]

# subset
D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc + male + marr + pira + twoearn,
                  data = sub)
```

Lastly, we specify the methods used in the ensemble with `create_method`:

```{r}
method = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf"),
 "xgboost" = create_method("xgboost")
)
```

## Run `nuisance_parameters`

Note: cross-fitting procedure can account for clusters within the data (provide `cluster` variable) and preserve the treatment ratios from full sample (specify `stratify=TRUE`). If both are provided, stratification on treatment is ignored due to computational constraints and feasibility issues.

Now, we are ready to run the main function:

```{r}
# Short stacking 
np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
                                X, Y, D, Z,
                                method = method, cf = 5, stacking = "short",
                                cluster = NULL, stratify = FALSE,
                                storeModels = "Memory", path = NULL, quiet = FALSE)
```

For a sanity check, let's estimate some target parameters using early `MLeffect` function and compare them to `dml_with_smoother` of `OutcomeWeights`:

```{r}
TargetParam <- MLeffect(Y, D, X, Z, 
                        NuPa.hat = np_short$nuisance_parameters,
                        estimators = c("PLR","PLR_IV","AIPW_ATE","Wald_AIPW"))

summary(TargetParam)
plot(TargetParam)
```

We work with only a fraction of `401k`, so the estimates are noisy.

Observe that the results are very close to those from the 5-fold `dml_with_smoother` routine:

```{r}
dml_5f = dml_with_smoother(Y,D,X,Z, n_cf_folds = 5, n_reps = 2)
results_dml_5f = summary(dml_5f)
plot(dml_5f)
```

We can also check the ensemble weights in the short stacking procedure. These are obtained by regressing the outcome on the individual cross-fitted ML learners using non-negative least squares:

```{r}
plot(np_short$numbers$ens_weights)
```

Note that for `D.hat.z0`, equal ensemble weights are used due to data peculiarities: $D$ and $Z$ are nearly identical in the subsample where $Z = 0$ ($D$ shows little variation as it is almost always 0).

## Extract smoother matrices S

Here comes the second function, which will later be moved to the `OutcomeWeights` package. It extracts outcome smoother matrices (for nuisance parameters starting with `Y.hat..`) using the trained models saved in the previous step:

```{r}
S <- NuisanceParameters::get_outcome_weights(
  np_object = np_short, 
  NuPa = c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"))

# Check if SY = Y_hat
check_pairs <- list(
  list(A = S$Y.hat_m %*% Y,         B = np_short$nuisance_parameters$Y.hat),
  list(A = S$Y.hat.d_m[[1]] %*% Y,  B = np_short$nuisance_parameters$Y.hat.d[, 1]),
  list(A = S$Y.hat.d_m[[2]] %*% Y,  B = np_short$nuisance_parameters$Y.hat.d[, 2]),
  list(A = S$Y.hat.z_m[[1]] %*% Y,  B = np_short$nuisance_parameters$Y.hat.z[, 1]),
  list(A = S$Y.hat.z_m[[2]] %*% Y,  B = np_short$nuisance_parameters$Y.hat.z[, 2]))

checks <- sapply(check_pairs, function(pair) {isTRUE(all.equal(as.numeric(pair$A), pair$B))})
all(checks)
```

Seeing `FALSE` should not throw us off: the current smoother matrix construction for XGBoost leaves a tiny wedge between weighted outcomes and estimated nuisance parameters. Once we allow for some slack by setting a tolerance level, the equality is restored:

```{r}
checks <- sapply(check_pairs, function(pair) {
  isTRUE(all.equal(as.numeric(pair$A), pair$B, tolerance = 1e-7))
})

all(checks)
```

At this point, the `get_outcome_weights` function is a bit of a misnomer: it gives smoother matrices instead of outcome weights. In the future, it will also take in the target parameters estimated in `MLeffects` or elsewhereâ€”and provide a vector of outcome weights.

### Standard stacking

Now, let's repeat for standard stacking and print the progress. Here, we first save it to disk and then load it back in to check that everything works:

```{r}
# Standard stacking 
np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat.z","D.hat","Z.hat"),
                                X, Y, D, Z,
                                method = method, cf = 5, stacking = 2,
                                cluster = NULL, stratify = FALSE,
                                storeModels = "Disk", quiet = FALSE, 
                                path = "/Users/romanrakov/Desktop")
```

The progress bar is useful for giving more information on what exactly is being done:

-   `cv = 1, 2, ...`: Each candidate learner $j$ is trained on the data excluding the current cross-validation hold-out fold, i.e., $T_k \setminus T_{k,v}$ in the notation of Ahrens et al. (2024)

-   `cv = .`: After cross-validating the individual learners within each cross-fitting fold, we re-fit our learners on the full training portion of the cross-fitting fold (full sample, fs)

-   `cv = .`: We use the learners fitted in `cv = .` and make predictions on the held-out part of the cross-fitting fold using ensemble weights obtained from `cv = 1, 2, ...`.

```{r}
S <- NuisanceParameters::get_outcome_weights(
  np_object = "/Users/romanrakov/Desktop/nuisance_models.rds", 
  NuPa = c("Y.hat","Y.hat.d","Y.hat.z"))

# Check if SY = Y_hat
check_pairs <- list(
  list(A = S$Y.hat_m %*% Y,         B = np_stand$nuisance_parameters$Y.hat),
  list(A = S$Y.hat.d_m[[1]] %*% Y,  B = np_stand$nuisance_parameters$Y.hat.d[, 1]),
  list(A = S$Y.hat.d_m[[2]] %*% Y,  B = np_stand$nuisance_parameters$Y.hat.d[, 2]),
  list(A = S$Y.hat.z_m[[1]] %*% Y,  B = np_stand$nuisance_parameters$Y.hat.z[, 1]),
  list(A = S$Y.hat.z_m[[2]] %*% Y,  B = np_stand$nuisance_parameters$Y.hat.z[, 2]))

checks <- sapply(check_pairs, function(pair) {
  isTRUE(all.equal(as.numeric(pair$A), pair$B, tolerance = 1e-7))})
all(checks)
```

We can also display the weights for standard stacking. The format is slightly different: for each nuisance parameter, weights are computed within every cross-fitting fold:

```{r}
plot(np_stand$numbers$ens_weights)
```

## Hyperparameter tuning

As of now, only `grf`'s regression forest can be tuned.

It can be done on full sample by specifying `arguments = list("tune_full")`..

```{r}
# ml = list(
#  "ols" = create_method("ols"),
#  "forest_grf" = create_method("forest_grf", arguments = list("tune_full"))
# )
# 
# time_short_tune_full <- system.time({
# np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
#                                 X, Y, D, Z, ml = ml, cf = 5, stacking = "short",
#                                 learner = c("t"), storeModels = "Memory")})
# 
# time_stand_tune_full <- system.time({
# np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
#                                 X, Y, D, Z, ml = ml, cf = 5, stacking = 2,
#                                 learner = c("t"), storeModels = "Memory")})
```

Or on the fold by specifying `arguments = list("tune_fold")`..

```{r}
# ml = list(
#  "ols" = create_method("ols"),
#  "forest_grf" = create_method("forest_grf", arguments = list("tune_fold"))
# )
# 
# # Measure execution time
# time_short_tune_fold <- system.time({
# np_short <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
#                                 X, Y, D, Z, ml = ml, cf = 5, stacking = "short",
#                                 learner = c("t"), storeModels = "No")})
# 
# time_stand_tune_fold <- system.time({
# np_stand <- nuisance_parameters(NuPa=c("Y.hat","Y.hat.d","Y.hat.z","D.hat","D.hat.z","Z.hat"),
#                                 X, Y, D, Z, ml = ml, cf = 5, stacking = 2,
#                                 learner = c("t"), storeModels = "No")})
```

The warnings appear because there is very little variation in $D$ when $Z = 0$.

And print the training time:

```{r}
# cat("Short stacking not tuned:    ", time_short_notune[3], "s
#      Standard stacking not tuned: ", time_stand_notune[3], "s
#      \nShort stacking full-tuned:   ", time_short_tune_full[3], "s  
#      Standard stacking full-tuned:", time_stand_tune_full[3], "s
#      \nShort stacking fold-tuned:   ", time_short_tune_fold[3], "s  
#     Standard stacking fold-tuned:", time_stand_tune_fold[3], "s")
```
