---
title: "Hyperparameter Tuning"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "09/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Hyperparameter Tuning}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r}
library(NuisanceParameters)
library(hdm)
```

This vignette briefly covers hyperparameter tuning supported by `NuisanceParameters`.

## Hyperparameter tuning

Selected learners, such as *grf*'s regression forest and *XGBoost*, support hyperparameter tuning. This can be done either on the full sample or "on-the-fold."

-   For the regression forest, *grf*'s default tuning routine is run. If `tuneLearners = "fold"`, tuning happens on the estimation part of the cross-fitting split, which is roughly $F$ times more computationally intensive than using the full sample.

-   *XGBoost* is tuned via a hyperband-style multi-armed bandit approach: it progressively allocates resources (boosting rounds) to the most promising configurations across successive evaluation rungs.

See details in `?tune_learners` and `?tune_xgb_hyperband`. See also [Bach et al. (2024)](https://arxiv.org/abs/2402.04674) for a discussion of hyperparameter tuning in Double Machine Learning.

Note that the base learners `ridge`, `plasso`, and `lasso` use cross-validation to select the regularization strength $Î»$ by default.

## Example using hdm's pension data

As before, we use the dataset that is provided in the `hdm` package. See [Chernozhukov and Hansen (2004)](#0) and Belloni et al. (2014) for details.

The sample contains 9,915 observations. We use a 25% subset to speed up computation:

```{r}
# see ?pension for details
data("pension", package = "hdm")

# subset
idx <- sample(nrow(pension), size = round(0.25 * nrow(pension)))
sub <- pension[idx, ]

D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc 
                  + male + marr + pira + twoearn, data = sub)
```

Define the out-of-sample root mean-squared error (RMSE) to compare performance across specifications:

```{r}
get_rmse <- function(Y.hat, Y) {
  sqrt(mean((Y - Y.hat)^2))
}
```

Base learners are registered as usual:

```{r}
methods = list(
 "ols" = create_method("ols"),
 "xgboost" = create_method("xgboost"),
 "forest_grf" = create_method("forest_grf")
 )
```

Set `tuneLearners = "full_sample"` to enable hyperparameter tuning on the full sample:

```{r}
time <- system.time({
  np <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = methods, cf = 3, stacking = "short", 
    storeModels = "Memory")
  })

time_tune <- system.time({
  np_fs_tnd <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = methods, cf = 3, stacking = "short", 
    storeModels = "Memory", tuneLearners = "full_sample")
  })

plot(np$numbers$ens_weights)
plot(np_fs_tnd$numbers$ens_weights)
```

By setting `tuneLearners = "fold"`, we perform on-the-fold hyperparameter tuning, which is expected to be more resource-intensive:

```{r}
time_fold_tune <- system.time({
  np_fold_tnd <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = methods, cf = 3, stacking = "short", 
    storeModels = "Memory", tuneLearners = "fold")
  })

# Display the runtime
time_df <- data.frame(
  Method = c("Default", "Full sample tuned", "On-the-fold tuned"),
  Time = c(time[1], time_tune[1], time_fold_tune[1]))

print(time_df)
```

Finally, the RMSE of the ensemble predictions, as well as of the individual learners, is calculated:

```{r}
ens_rmse <- c(Default = get_rmse(np$nuisance_parameters$Y.hat, Y),
              Tuned_fs = get_rmse(np_fs_tnd$nuisance_parameters$Y.hat, Y),
              Tuned_fold = get_rmse(np_fold_tnd$nuisance_parameters$Y.hat, Y))

rmse_results <- rbind(
  "Default" = apply(np$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y),
  "Tuned (full sample)" = apply(np_fs_tnd$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y),
  "Tuned (fold)" = apply(np_fold_tnd$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y)
)

cat("Ensemble:\n"); print(round(ens_rmse, 4))
cat("\nBase learners:\n"); print(round(rmse_results, 4))
```
