---
title: "Hyperparameter Tuning"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "09/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Hyperparameter Tuning}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r}
library(NuisanceParameters)
library(hdm)
```

This vignette covers hyperparameter tuning as implemented in `NuisanceParameters`.

## Hyperparameter tuning

Hyperparameter tuning improves predictive performance by adapting learners to the structure of the data.

In `NuisanceParameters`, it is supported by two learners: GRFs regression forest and XGBoost—and can be done either on the full sample or *on-the-fold*. In the latter, tuning takes place within each estimation part of the cross-fitting split, making it roughly as many times more computationally intensive as there are cross-fitting folds.

-   For the regression forest, GRF's [default tuning routine](https://grf-labs.github.io/grf/reference/regression_forest.html) is used.

-   XGBoost is tuned via a hyperband-style multi-armed bandit approach: it progressively allocates resources (boosting rounds) to the most promising configurations across successive evaluation rungs.

See details in `?tune_learners` and `?tune_xgb_hyperband`. See also [Bach et al. (2024)](https://arxiv.org/abs/2402.04674) for a discussion of hyperparameter tuning in Double Machine Learning.

Note that the base learners `ridge`, `plasso`, and `lasso` use cross-validation to select the regularization strength $λ$ by default.

## Tuning and out-of-sample performance

As before, we use the dataset that is provided in the `hdm` package. See Chernozhukov and Hansen (2004) and Belloni et al. (2014) for details.

The sample contains 9,915 observations. We use a 25% subset to speed up computation:

```{r}
# see ?pension for details
data("pension", package = "hdm")

# subset
set.seed(123)
idx <- sample(nrow(pension), size = round(0.25 * nrow(pension)))
sub <- pension[idx, ]

D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc 
                  + male + marr + pira + twoearn, data = sub)
```

Define the out-of-sample root mean-squared error (RMSE) to compare performance across specifications:

```{r}
get_rmse <- function(Y.hat, Y) {
  sqrt(mean((Y - Y.hat)^2))
}
```

When registering base learners, we additionally specify `tuning = "full_sample"` or `tuning = "fold"` (with `"no"` as the default):

```{r}
mtd = list(
 "ols" = create_method("ols"),
 "xgboost" = create_method("xgboost"),
 "forest_grf" = create_method("forest_grf")
 )

mtd_full = list(
 "ols" = create_method("ols"),
 "xgboost_full" = create_method("xgboost", tuning = "full_sample"),
 "forest_grf_full" = create_method("forest_grf", tuning = "full_sample")
 )

mtd_fold = list(
 "ols" = create_method("ols"),
 "xgboost_fold" = create_method("xgboost", tuning = "fold"),
 "forest_grf_fold" = create_method("forest_grf", tuning = "fold")
 )
```

The main function remains identical to the case with no tuning.

```{r}
time <- system.time({
  np <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = mtd, cf = 3, stacking = "short", 
    store_models = "memory")
  })

time_tune <- system.time({
  np_fs_tnd <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = mtd_full, cf = 3, stacking = "short", 
    store_models = "memory")
  })

time_fold_tune <- system.time({
  np_fold_tnd <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = mtd_fold, cf = 3, stacking = "short", 
    store_models = "memory")
  })

# Display the runtime
time_df <- data.frame(
  Method = c("Default", "Full sample tuned", "On-the-fold tuned"),
  Time = c(time[1], time_tune[1], time_fold_tune[1]))

print(time_df)
```

To see if performance gets any better, the RMSE of the ensemble predictions and the individual learners is calculated:

```{r}
ens_rmse <- c(Default = get_rmse(np$nuisance_parameters$Y.hat, Y),
              Tuned_fs = get_rmse(np_fs_tnd$nuisance_parameters$Y.hat, Y),
              Tuned_fold = get_rmse(np_fold_tnd$nuisance_parameters$Y.hat, Y))

rmse_results <- rbind(
  "Default" = apply(np$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y),
  "Tuned (full sample)" = apply(np_fs_tnd$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y),
  "Tuned (fold)" = apply(np_fold_tnd$models$Y.hat_m$ens_object$cf_preds, 2, get_rmse, Y))

rmse_results <- cbind(rmse_results, Ensemble = ens_rmse)
print(rmse_results)
```

Tuned and default learners can, of course, be combined:

```{r}
methods = list(
 "ols" = create_method("ols"),
 "xgboost" = create_method("xgboost"),
 "xgboost_full" = create_method("xgboost", tuning = "full_sample"),
 "xgboost_fold" = create_method("xgboost", tuning = "fold"),
 "forest_grf" = create_method("forest_grf"),
 "forest_grf_full" = create_method("forest_grf", tuning = "full_sample"),
 "forest_grf_fold" = create_method("forest_grf", tuning = "fold")
 )

np <- nuisance_parameters(
    NuPa = "Y.hat", X = X, Y = Y, D = D, 
    methods = methods, cf = 3, stacking = "short", 
    store_models = "memory")

plot(np$numbers$ens_weights)
```

```{r}
cf_preds <- np$models$Y.hat_m$ens_object$cf_preds

rmse <- c(Ensemble=get_rmse(np$nuisance_parameters$Y.hat, Y), apply(cf_preds, 2, get_rmse, Y))
print(as.matrix(rmse))
```
