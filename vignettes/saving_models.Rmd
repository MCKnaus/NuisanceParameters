---
title: "Saving models"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "10/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Saving models}
  %\VignetteEngine{knitr::rmarkdown}
---

## Introduction

This notebook briefly reviews three options for saving the individual trained models:

1.  `"memory"` — saves the models within the `NuisanceParameters` object returned by the `nuisance_parameters()` function,
2.  `"disk"` — saves the models locally at a user-specified path, and
3.  `"no"` — does not save the models.

## Why save base learners

These models are trained within each cross-fitting split and are used for out-of-sample nuisance parameter prediction. The predictions are then combined into an ensemble learner. For each cross-fitting fold $F$, there are $M$ trained base learners.

The common use case of the package is to deliver the estimated nuisance parameters, which are then used downstream—for example, to obtain an estimate of a target parameter of interest.

However, one prominent reason to save the models is to extract smoother matrices (see [Knaus, 2024](https://arxiv.org/abs/2411.11559)), for which the trained models are essential.

## Data

As before, we use the dataset that is provided in the `hdm` package. See Chernozhukov and Hansen (2004) and Belloni et al. (2014) for details.

The sample consists of 9,915 observations. To speed up the running time, we use a 25% subset of the data.

```{r}
library(hdm)
library(OutcomeWeights)
library(NuisanceParameters)

# Find dataset description if you type ?pension in console
data("pension", package = "hdm")

set.seed(123)
idx <- sample(nrow(pension), size = round(0.25 * nrow(pension)))
sub <- pension[idx, ]

D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc
                  + male + marr + pira + twoearn, data = sub)
```

We will specify a generic list of base learners for the ensemble. You can find the full list of available learners and their default settings by typing `?create_method` in the console. For more details on the different types of stacking and hyperparameter tuning, see the *Stacking* and *Hyperparameter* *Tuning* notebooks.

```{r}
methods = list(
 "ols" = create_method("ols"),
 "rlasso" = create_method("rlasso"),
 "forest_grf" = create_method("forest_grf"), 
 "xgboost" = create_method("xgboost")
 )
```

`nuisance_parameters` carries out cross-fitted estimation of nuisance parameters. If more than one method is provided, it will stack them together, creating an ensemble learner.

In this example, we use 3-fold cross-fitting and apply short stacking.

There are three options for storing the trained model output.

## Options for storing the models

### Memory

The first option is `"memory"`:

```{r}
np <- nuisance_parameters(NuPa = c("Y.hat","Y.hat.d","D.hat","Z.hat"),
                          X = X, Y = Y, D = D, Z = Z,
                          methods = methods, cf = 3, stacking = "short",
                          store_models = "memory", path = NULL)

# Check the object size
format(object.size(np$models), units = "MB")

fold_models <- np$models$Y.hat_m$ens_object$ens_models[[1]]
sapply(fold_models, function(x) format(object.size(x), units = "MB"))
```

For our sample (\~2.5k observations) with 3-fold cross-fitting, the total size of the models exceeds 1 gigabyte, with over 99% of the memory used to store the grf's random forest.

If the trained models are not immediately needed for downstream tasks, this motivates two alternative solutions.

### Saving to disk

Models can be saved to a user-specified path by selecting `store_models = "disk"`.

```{r}
# Define a temporary directory
tmpdir <- tempdir()

np <- nuisance_parameters(NuPa = c("Y.hat","Y.hat.d","D.hat","Z.hat"),
                          X = X, Y = Y, D = D, Z = Z,
                          methods = methods, cf = 3, stacking = "short",
                          store_models = "disk", path = tmpdir)
```

Smoother matrices are then extracted simply by providing the saved models to `get_smoother_weights`:

```{r}
object <- readRDS(file.path(tmpdir, "nuisance_models.rds"))

S <- get_smoother_weights(object = object, NuPa = "Y.hat")

# Check if S*Y = Y_hat
all.equal(as.numeric(S$Y.hat %*% Y), np$nuisance_parameters$Y.hat, tolerance = 1e-7)
```

Another use of saved models is to run post-estimation, method-specific routines.

Having run grf's regression forest, we might be interested in obtaining a measure of variable importance—that is, how often feature *i* was split at each depth in the forest.

Let's do exactly that for the outcome nuisance parameter, $\hat{E}(Y \mid X)$, in the first cross-fitting fold:

```{r}
m <- readRDS(file.path(tmpdir, "nuisance_models.rds"))
grf_object <- m[["models"]][["Y.hat_m"]][["ens_object"]][["ens_models"]][[1]][["forest_grf"]]

vi <- grf::variable_importance(grf_object)
df <- data.frame(Importance = vi, row.names = colnames(X))
print(df)
```

### Not saving

Alternatively, models can be completely omitted from the returned object, both to save space and to reduce the total computation time:

```{r}
np <- nuisance_parameters(NuPa = "Y.hat", X = X, Y = Y, D = D, Z = Z, 
                          stacking = "short", methods = methods, cf = 3,
                          store_models = "no")
str(np, 1)
```
