---
title: "Estimation"
subtitle: ""
author:
  - Michael Knaus
  - Stefan Glaisner
  - Roman Rakov
date: "09/25"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Estimation}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r}
library(NuisanceParameters)
library(hdm)
```

## Introduction

This vignette provides additional details on the estimation capabilities of the `NuisanceParameters` package.

Specifically, it covers:

-   flexible method creation
-   short and standard stacking
-   hyperparameter tuning for selected learners

## Registering base learners

The package offers a rich set of base learners. A detailed description of available learners and their default settings can be found in `?create_method`.

You can register a list of methods in two main ways.

A minimal working definition of methods can be specified as:

```{r}
methods = list(
 "xgboost" = create_method("xgboost"),
 "rlasso" = create_method("rlasso"),
 "logit" = create_method("logit"),
 "logit_nnet" = create_method("logit_nnet")
 )
```

The estimation routine of `nuisance_parameters` automatically checks the defined methods and excludes those that are not suitable for a given nuisance parameter. For example, a logit specification will not be applied when estimating the outcome nuisance parameter, $E[Y \mid X = x]$.

When a multivalued propensity score is estimated, users must explicitly specify the estimation technique by choosing among `"one-vs-one"`, `"one-vs-rest"`, or `"multiclass"`.

```{r}
methods = list(
 "xgboost" = create_method("xgboost"),
 "rlasso" = create_method("rlasso"),
 "logit" = create_method("logit", multinomial = "one-vs-one"),
 "prob_forest" = create_method("prob_forest", multinomial = "multiclass")
 )
```

More details on these routines are given in the vignette on multivalued treatments.

Base learners can also be defined in a more flexible way, allowing finer control over the learner selection process:

```{r}
methods = list(
  "Y.hat" = list(
    "forest_grf" = create_method("forest_grf"),
    "rlasso" = create_method("rlasso")
    ),
  "D.hat" = list(
    "logit_ovo" = create_method("logit", multinomial = "one-vs-one"),
    "logit_nnet_ovr" = create_method("logit_nnet", multinomial = "one-vs-rest")
    ), 
  "D.hat.z" = list(
    "prob_forest" = create_method("prob_forest", multinomial = "multiclass"),
    "xgboost_prop_ovr" = create_method("xgboost_prop", multinomial = "multiclass")
    ), 
  "Y.hat.d" = list(
    "ols" = create_method("ols"),
    "forest_grf" = create_method("forest_grf", arguments = list("num.trees" = 1000)),
    "knn" = create_method("knn", arguments = list("k" = 3))
    )
  )
```

A logical vector (`x_select`) can be supplied to indicate which variables to use in the estimation. For instance, tree-based methods typically exclude the interaction terms used by Lasso.

## Short- and Standard-Stacking

Learners can be combined into an ensemble through stacking. Following Ahrens et al. (2024), the package implements two variants: standard stacking and short stacking.

#### Standard stacking

In standard stacking, each learner is trained and validated within the folds of the cross-fitting procedure. A second layer of cross-validation is used to determine ensemble weights, which are estimated with non-negative least squares. This method is statistically solid but can be very computationally expensive: with $K$ folds, $V$ validation splits, and $M$ learners, complexity grows roughly with $K \times V \times M$.

#### Short stacking

Short stacking avoids the extra validation step. Instead of re-splitting the data, it uses the cross-fitted predictions directly to estimate ensemble weights. This keeps the key benefit of cross-fitting (avoiding own-observation bias) while cutting complexity down to about $K \times M$.

Ahrens et al. (2024) suggest short stacking for most applications, especially when the number of learners $M$ is small relative to the sample size.

In the package, switching between the two forms of stacking is straightforward: adjust the `stacking` argument by either providing the string `"short"` or by supplying the number of folds to be used for cross-validating ensemble weights within each cross-fitting fold.

## Hyperparameter tuning

Selected learners (`grf`'s regression forest and `xgboost`) support hyperparameter tuning. See details in `?tune_learners` and `?tune_xgb_hyperband`.

Tuning can be run on the full sample or within each fold. Note that fold-level tuning is computationally more demanding. See [Bach et al. (2024)](https://arxiv.org/abs/2402.04674) for a discussion of hyperparameter tuning in Double Machine Learning.

## Example using hdm's pension dataset

We use pension dataset that is provided in the `hdm` package. The data was used in [Chernozhukov and Hansen (2004)](#0) and Belloni et al. (2014) where further details can be found. They investigate the effect of participation in the employer-sponsored 401(k) retirement savings plan on net assets (`net_tfa`), using 401(k) eligibility (`e401`) as an instrument for 401(k) participation (`p401`).

The sample is drawn from the 1991 Survey of Income and Program Participation (SIPP) and contains 9,915 observations. We use a subset to speed up computation:

```{r}
# see ?pension for details
data("pension", package = "hdm")

# subset
idx <- sample(nrow(pension), size = round(0.25 * nrow(pension)))
sub <- pension[idx, ]

D <- sub$p401
Z <- sub$e401
Y <- sub$net_tfa
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc 
                  + male + marr + pira + twoearn, data = sub)

methods = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf"),
 "xgboost" = create_method("xgboost")
 )

methods_tune = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf", arguments = c("tune_full_sample")),
 "xgboost" = create_method("xgboost", arguments = c("tune_full_sample"))
 )
```

Define the out-of-sample root mean-squared error (RMSE) to compare performance across specifications:

```{r}
calculate_rmse <- function(Y.hat, Y) {
  sqrt(mean((Y - Y.hat)^2))
}
```

And estimate the specified nuisance parameters:

```{r}
time_short <- system.time({
  np <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods, cf = 5, stacking = "short", storeModels = "Memory")
  })

time_short_tune <- system.time({
  np_tune <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods_tune, cf = 5, stacking = "short", storeModels = "Memory")
  })

plot(np$numbers$ens_weights)
plot(np_tune$numbers$ens_weights)
```

We can also compute the RMSE of the ensemble predictions as well as of the individual learners:

```{r}
ens_rmse <- c(Default = calculate_rmse(np$nuisance_parameters$Y.hat, Y),
              Tuned = calculate_rmse(np_tune$nuisance_parameters$Y.hat, Y))

learner_rmse   <- apply(np$models$Y.hat_m$ens_object$cf_preds, 2, calculate_rmse, Y)
learner_rmse_t <- apply(np_tune$models$Y.hat_m$ens_object$cf_preds, 2, calculate_rmse, Y)

cat("Ensemble:\n"); print(round(ens_rmse, 4))
cat("\nDefault learners:\n"); print(round(learner_rmse, 4))
cat("\nTuned learner:\n"); print(round(learner_rmse_t, 4))
```

We now turn to standard stacking, using 3-fold cross-validation. Note that standard stacking produces a set of ensemble weights for each cross-fitting fold.

```{r}
time_stand <- system.time({
  np <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods, cf = 5, stacking = 3, storeModels = "No")
  })

time_stand_tune <- system.time({
  np_tune <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods_tune, cf = 5, stacking = 3, storeModels = "No")
  })

plot(np$numbers$ens_weights)
plot(np_tune$numbers$ens_weights)
```

Finally, we perform on-the-fold hyperparameter tuning, which is expected to be the most resource-intensive:

```{r}
methods_tune = list(
 "ols" = create_method("ols"),
 "forest_grf" = create_method("forest_grf", arguments = c("tune_on_fold")),
 "xgboost" = create_method("xgboost", arguments = c("tune_on_fold"))
 )

time_short_fold_tune <- system.time({
  np <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods_tune, cf = 5, stacking = "short", storeModels = "No")
  })

time_stand_fold_tune <- system.time({
  np_tune <- nuisance_parameters(
    NuPa = c("Y.hat", "D.hat"), X = X, Y = Y, D = D, 
    methods = methods_tune, cf = 5, stacking = 3, storeModels = "No")
  })

# Display the runtime
time_df <- data.frame(
  Method = c("Short", "Short (sample tuned)", "Short (fold tuned)",
             "Standard", "Standard (sample tuned)", "Standard (fold tuned)"),
  Time = c(time_short[1], time_short_tune[1], time_short_fold_tune[1],
           time_stand[1], time_stand_tune[1], time_stand_fold_tune[1]))

print(time_df)
```

### 
