---
bibliography: vignettes/assets/NuisanceParameters_refs.bib
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.path = "man/figures/README-"
)
```

# NuisanceParameters <img src="assets/temp_logo.png" align="right" height="139"/>

<!-- # NuisanceParameters <img src='assets/NuisanceParameters.png' align="right" height="139" /> -->

<!-- badges: start -->

<!-- [![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) -->

[![License](https://img.shields.io/badge/license-GPL--3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0.en.html) [![Project_Status](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active) <!-- [![Downloads_Total](https://cranlogs.r-pkg.org/badges/grand-total/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) --> <!-- [![Downloads_Monthly](https://cranlogs.r-pkg.org/badges/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) -->

<!-- badges: end -->

`NuisanceParameters` lets you estimate conditional expectations that can later be used to estimate target causal parameters of interest. It implements the double/debiased machine learning framework of Chernozhukov et al. (2018).

A defining feature of the package is its use of supervised machine learning ("grey box") algorithms, which — following a general framework established in [Knaus (2024)](https://arxiv.org/abs/2411.11559) — have a representation as a linear combination of observed outcomes: $\hat{\tau} = \sum_{i=1}^N \omega_i Y_i$. These weights can be extracted and used in established routines for classic weighting estimators.

This package is part of an envisaged trilogy, where each package can be seamlessly integrated with the previous one, but also used as a stand-alone unit:

1.  `NuisanceParameters` – flexibly estimates $m(X):=\mathbb{E}[Y \mid X]$, $m_d(d,X):=\mathbb{E}[Y \mid D = d, X]$, $e(X) := \mathbb{P}[D \mid X]$, and other user-specified nuisance parameters.
2.  `MLeffects` – combines estimated nuisance parameters ($\hat{m}(X)$, $\hat{m}_d(d,X)$, $\hat{e}(X)$, …) in the doubly robust (DR) score to estimate a target parameter $\tau$.
3.  `OutcomeWeights` – extracts the smoother matrices $S$ and calculates the outcome weights $\omega$ behind the nuisance and target parameters. The weights can be used to study estimator properties or to check covariate balance (see [Knaus, 2024](https://arxiv.org/abs/2411.11559)).

Among other features, `NuisanceParameters` offers ensemble estimation (using short and standard stacking), works with binary and multivalued treatments, and allows for a rich selection of base learners.

### In progress

The package is work in progress. Find here the current state (suggestions welcome):

-   [x] Compatibility with [`OutcomeWeights`](https://github.com/MCKnaus/OutcomeWeights) package
    -   [x] Create a separate `get_outcome_weights` function that takes the objects produced by `nuisance_parameters` as inputs and provides the user with a list of $N \times N$ smoother matrices for all outcome regressions produced in `NuisanceParameters`, or raise a flag (e.g., for Lasso)
    -   [x] Integrate it with `OutcomeWeights` package
-   [ ] Storage options
    -   [x] Allow the user to choose where to store the models: "No" (just the nuisance parameters in the output), "Memory" (keep all trained models in the object), or "Disk" (write them to disk)
    -   [ ] Store more efficiently with sparse matrices
-   [ ] Allow for more general treatment types
    -   [x] Binary
    -   [x] Multivalued
    -   [ ] Continuous

### Example: Nuisance Parameters with Short-Stacking

The code below shows how desired nuisance parameters can be flexibly estimated. The data was used in [Chernozhukov and Hansen (2004)](https://econpapers.repec.org/article/tprrestat/v_3a86_3ay_3a2004_3ai_3a3_3ap_3a735-751.htm), who investigated the effect of participation in the employer-sponsored 401(k) retirement savings plan (`p401`) on net assets (`net_tfa`).

```{r}
library(NuisanceParameters)
library(hdm)

set.seed(123)
idx <- sample(nrow(pension), size = round(0.5 * nrow(pension)))
sub <- pension[idx, ]

# Find dataset description by typing ?pension in console
Y <- sub$net_tfa
D <- sub$p401
Z <- sub$e401
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc 
                  + male + marr + pira + twoearn, data = sub)

methods = list(
 "ols" = create_method("ols"),
 "plasso" = create_method("plasso"),
 "xgboost" = create_method("xgboost"),
 "forest_grf" = create_method("forest_grf")
 )

np <- nuisance_parameters(NuPa = c("Y.hat","Y.hat.d","Y.hat.z","D.hat"),
                          X = X, Y = Y, D = D, Z = Z,
                          methods = methods, cf = 5, stacking = "short",
                          cluster = NULL, stratify = TRUE, ensemble_type = "nnls",
                          store_models = "memory", path = NULL, quiet = TRUE
                          )

# Estimated nuisance parameters
lapply(np$nuisance_parameters, head)

# Ensemble weights
plot(np$numbers$ens_weights)
```

### Learn More about `NuisanceParameters`

See our vignettes to learn more:

-   `vignette("quick_start")` is a more detailed introduction to `NuisanceParameters`
-   `vignette("stacking")` discusses short- and standard-stacking
-   `vignette("multivalued")` illustrates support of multivalued treatments
-   `vignette("saving_models")` reviews three options for saving the trained models
-   `vignette("hyperparameter_tuning")` covers hyperparameter tuning

### Supported Methods

Base learners can be adjusted by providing method-specific arguments in `create_method`. For binary outcomes, binary treatments, and any instrument, all learners are available. Some learners are not implemented for certain nuisance parameters and will be filtered out: restrictions apply for continuous outcomes (column 3) and multivalued treatments (column 4):

| Method | Description | Outcome NuPa | Treatment NuPa |
|------------------|------------------|------------------|------------------|
| `mean` | Mean difference. | Cont. & Binary | Binary |
| `ols` | Ordinary Least Squares. | Cont. & Binary | Binary |
| `ridge` | Ridge regression via `glmnet`. | Cont. & Binary | Binary |
| `plasso` | Post-Lasso via `plasso` (built on `glmnet`). | Cont. & Binary | Binary |
| `lasso` | Lasso via `glmnet`. | Cont. & Binary | Binary |
| `rlasso` | (Post-) Lasso via `hdm` with theory-based penalty. | Cont. & Binary | Binary |
| `forest_grf` | Regression forest via `grf`. Can be tuned with grf’s tuning routines. | Cont. & Binary | Binary |
| `ranger_fit` | Regression forest via `ranger` (`keep.inbag = TRUE`). Supports tuning via [`tuneRanger`](https://github.com/PhilippPro/tuneRanger?tab=readme-ov-file). | Cont. & Binary | Binary |
| `xgboost` | Gradient boosting via `xgboost` (100 rounds); `alpha = 0`, `subsample = 1`, `max_delta_step = 0`, and `base_score = mean(Y)` are suggested as defaults but can be overwritten (see `?create_method` for details). Supports tuning (see `?xgboost_tune`). | Cont. & Binary | Binary |
| `knn` | k-Nearest Neighbors via `FastKNN`. | Cont. & Binary | Binary |
| `forest_drf` | Distributional random forest via `drf` (uses FourierMMD splits). | Cont. & Binary | Binary |
| `glm` | Logit and probit via `glm`. | Binary | Binary |
| `logit` | Logistic regression via `glmnet`. Uses `family = "binomial"` for binary outcomes, `family = "multinomial"` for multiclass outcomes. | Binary | Binary & Multiclass-Native |
| `logit_nnet` | Logistic regression via `nnet::multinom()`. | Binary | Binary & Multiclass-Native |
| `nb_gaussian` | Gaussian Naive Bayes via `naivebayes::gaussian_naive_bayes()`. | Binary | Binary & Multiclass-Native |
| `nb_bernoulli` | Bernoulli Naive Bayes via `naivebayes::naive_bayes()`. | Binary | Binary & Multiclass-Native |
| `xgboost_prop` | Gradient boosting for propensity scores via `xgboost`. Uses `objective = "binary:logistic"` for binary and `objective = "multi:softprob"`. for multiclass treatments. Defaults to 100 rounds. | Binary | Binary & Multiclass-Native |
| `svm` | SVM via `e1071::svm()` (radial kernel). | Binary | Binary & Multiclass-Wrapper |
| `prob_forest` | Probability forest via `grf`. | Binary | Binary & Multiclass-Native |
| `ranger_prop` | Random forest classifier via `ranger` (`probability = TRUE`). Supports tuning via [`tuneRanger`](https://github.com/PhilippPro/tuneRanger?tab=readme-ov-file). | Binary | Binary & Multiclass-Native |
| `knn_prop` | k-NN classifier via `kknn::train.kknn()`. | Binary | Binary & Multiclass-Native |

### Bug reports & support

The development version will soon be available using the `devtools` package:

``` r
library(devtools)
install_github(repo="MCKnaus/NuisanceParameters")
```

For reporting a bug, simply [open an issue](https://github.com/stefan-1997/NuisanceParameters/issues/new) on GitHub. For personal contact, you can write an email to michael.knaus\@uni-tuebingen.de.

### References

Knaus, M. C. (2024). Treatment effect estimators as weighted outcomes, [arXiv:2411.11559](https://arxiv.org/abs/2411.11559)
