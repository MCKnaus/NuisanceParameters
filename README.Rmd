---
bibliography: vignettes/assets/NuisanceParameters_refs.bib
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.path = "man/figures/README-"
)
```

# NuisanceParameters <img src="assets/temp_logo.png" align="right" height="139"/>

<!-- # NuisanceParameters <img src='assets/NuisanceParameters.png' align="right" height="139" /> -->

<!-- badges: start -->

<!-- [![CRAN_Status_Badge](https://www.r-pkg.org/badges/version/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) -->

[![License](https://img.shields.io/badge/license-GPL--3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0.en.html) [![Project_Status](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active) <!-- [![Downloads_Total](https://cranlogs.r-pkg.org/badges/grand-total/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) --> <!-- [![Downloads_Monthly](https://cranlogs.r-pkg.org/badges/NuisanceParameters)](https://CRAN.R-project.org/package=NuisanceParameters) -->

<!-- badges: end -->

`NuisanceParameters` lets you estimate conditional expectations that can later be used to estimate target causal parameters of interest. It implements the double/debiased machine learning framework of Chernozhukov et al. (2018).

A defining feature of the package is its use of supervised machine learning ("grey box") algorithms, which — following a general framework established in [Knaus (2024)](https://arxiv.org/abs/2411.11559) — have a representation as a linear combination of observed outcomes: $\hat{\tau} = \sum_{i=1}^N \omega_i Y_i$. These weights can be extracted and used in established routines for classic weighting estimators.

This package is part of an envisaged trilogy, where each package can be seamlessly integrated with the previous one, but also used as a stand-alone unit:

1.  `NuisanceParameters` – flexibly estimates $m(X):=\mathbb{E}[Y \mid X]$, $m_d(d,X):=\mathbb{E}[Y \mid D = d, X]$, $e(X) := \mathbb{P}[D \mid X]$, and other user-specified nuisance parameters.
2.  `MLeffects` – combines estimated nuisance parameters ($\hat{m}(X)$, $\hat{m}_d(d,X)$, $\hat{e}(X)$, …) in the doubly robust (DR) score to estimate a target parameter $\tau$.
3.  `OutcomeWeights` – extracts the smoother matrices $S$ and calculates the outcome weights $\omega$ behind the nuisance and target parameters. The weights can be used to study estimator properties or to check covariate balance (see [Knaus, 2024](https://arxiv.org/abs/2411.11559)).

Among other features, `NuisanceParameters` offers ensemble estimation (using short and standard stacking), works with binary and multivalued treatments, and allows for a rich selection of base learners.

### In progress

The package is work in progress. Find here the current state (suggestions welcome):

-   [x] Compatibility with [`OutcomeWeights`](https://github.com/MCKnaus/OutcomeWeights) package
    -   [x] Create a separate `get_outcome_weights` function that takes the objects produced by `nuisance_parameters` as inputs and provides the user with a list of $N \times N$ smoother matrices for all outcome regressions produced in `NuisanceParameters`, or raise a flag (e.g., for Lasso)
    -   [x] Integrate it with `OutcomeWeights` package
-   [ ] Storage options
    -   [x] Allow the user to choose where to store the models: "No" (just the nuisance parameters in the output), "Memory" (keep all trained models in the object), or "Disk" (write them to disk)
    -   [ ] Store more efficiently with sparse matrices
-   [ ] Allow for more general treatment types
    -   [x] Binary
    -   [x] Multivalued
    -   [ ] Continuous

### Example: Nuisance Parameters with Short-Stacking

The code below shows how desired nuisance parameters can be flexibly estimated. The data was used in [Chernozhukov and Hansen (2004)](https://econpapers.repec.org/article/tprrestat/v_3a86_3ay_3a2004_3ai_3a3_3ap_3a735-751.htm), who investigated the effect of participation in the employer-sponsored 401(k) retirement savings plan (`p401`) on net assets (`net_tfa`).

```{r}
library(NuisanceParameters)
library(hdm)

set.seed(123)
idx <- sample(nrow(pension), size = round(0.5 * nrow(pension)))
sub <- pension[idx, ]

# Find dataset description if you type ?pension in console
Y <- sub$net_tfa
D <- sub$p401
Z <- sub$e401
X <- model.matrix(~ 0 + age + db + educ + fsize + hown + inc 
                  + male + marr + pira + twoearn, data = sub)

methods = list(
 "ols" = create_method("ols"),
 "plasso" = create_method("plasso"),
 "xgboost" = create_method("xgboost"),
 "forest_grf" = create_method("forest_grf")
 )

np <- nuisance_parameters(NuPa = c("Y.hat","Y.hat.d","Y.hat.z","D.hat"),
                          X = X, Y = Y, D = D, Z = Z,
                          methods = methods, cf = 5, stacking = "short",
                          cluster = NULL, stratify = TRUE, ensemble_type = "nnls",
                          store_models = "memory", path = NULL, quiet = TRUE
                          )

# Estimated nuisance parameters
lapply(np$nuisance_parameters, head)

# Ensemble weights
plot(np$numbers$ens_weights)
```

### Learn More about `NuisanceParameters`

See our vignettes to learn more:

-   `vignette("quick_start")` is a more detailed introduction to `NuisanceParameters`
-   `vignette("stacking")` discusses short- and standard-stacking
-   `vignette("multivalued")` illustrates support of multivalued treatments
-   `vignette("saving_models")` reviews three options for saving the trained models
-   `vignette("hyperparameter_tuning")` covers hyperparameter tuning

### Supported Methods

Base learners can be adjusted by providing method-specific arguments in `create_method`. Some learners are not implemented for certain nuisance parameters and will be filtered out; see the table:

| Method | Description | Outcome NuPa | D/Z Capability |
|------------------|------------------|------------------|------------------|
| `mean` | Mean difference. | Yes | Binary |
| `ols` | Ordinary Least Squares. | Yes | Binary |
| `ridge` | Ridge regression via `glmnet` with 10-fold CV to select λ. | Yes | Binary |
| `plasso` | Post-Lasso via `plasso` (built on `glmnet`), same tuning as Ridge. | Yes | Binary |
| `lasso` | Lasso via `glmnet`, same tuning as Ridge. | Yes | Binary |
| `rlasso` | (Post-) Lasso via `hdm` with theory-based penalty. Defaults: $c = 1.1$, $\gamma = 0.1 / \log(n)$. | Yes | Binary |
| `forest_grf` | Regression forest via `grf` (2000 trees, honesty = TRUE). Supports full-sample & fold tuning using grf’s tuning routines. | Yes | Binary |
| `xgboost` | Gradient boosting via `xgboost` (100 rounds; some hyperparameters are fixed - see `?create_method`). Supports full-sample & fold tuning (see `?tune_xgb_hyperband`) | Yes | Binary |
| `knn` | k-Nearest Neighbors via `FastKNN` (default k = 10). | Yes | Binary |
| `forest_drf` | Distributional random forest via `drf` (2000 trees, FourierMMD splits). | Yes | Binary |
| `glm` | Logit and probit via `glm`. | No | Binary |
| `logit` | Logistic regression via `glmnet`. Uses `family = "binomial"` for binary outcomes, `family = "multinomial"` for multiclass outcomes. | No | Binary & Multiclass-Native |
| `logit_nnet` | Logistic regression via `nnet::multinom()`. | No | Binary & Multiclass-Native |
| `nb_gaussian` | Gaussian Naive Bayes via `naivebayes::gaussian_naive_bayes()`. | No | Binary & Multiclass-Native |
| `nb_bernoulli` | Bernoulli Naive Bayes via `naivebayes::naive_bayes()`. | No | Binary & Multiclass-Native |
| `xgboost_prop` | Gradient boosting for propensity scores via `xgboost`. Uses `objective = "binary:logistic"` or `objective = "multi:softprob"`. Default 100 rounds. | No | Binary & Multiclass-Native |
| `svm` | SVM via `e1071::svm()` (radial kernel). | No | Binary & Multiclass-Wrapper |
| `prob_forest` | Probability forest via `grf` (2000 trees). | No | Binary & Multiclass-Native |
| `ranger` | Random forest classifier via `ranger` (500 trees). | No | Binary & Multiclass-Native |
| `knn_prop` | k-NN classifier via `kknn::train.kknn()` with leave-one-out cross-validation of k. | No | Binary & Multiclass-Native |

### Bug reports & support

The development version will soon be available using the `devtools` package:

``` r
library(devtools)
install_github(repo="MCKnaus/NuisanceParameters")
```

For reporting a bug, simply [open an issue](https://github.com/stefan-1997/NuisanceParameters/issues/new) on GitHub. For personal contact, you can write an email to michael.knaus\@uni-tuebingen.de.

### References

Knaus, M. C. (2024). Treatment effect estimators as weighted outcomes, [arXiv:2411.11559](https://arxiv.org/abs/2411.11559)
